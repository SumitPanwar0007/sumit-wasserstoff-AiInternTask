{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rag\n"
     ]
    }
   ],
   "source": [
    "print(\"rag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for attention_removed.pdf:\n",
      "provided proper attribution is provided google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works attention is all you need ashish vaswani google brain avaswani google comnoam shazeer google brain noam google comniki parmar google research nikip google comjakob\n",
      "\n",
      "Keywords for attention_removed.pdf: attention, google, input, model, models, neural, output, positions, sequence, transformer\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Summary for ML.pdf:\n",
      "see discussions st ats and author pr ofiles f or this public ation at https www researchgate ne t public ation 339031674 getting started with machine learning ml article februar y 2020 citation 1reads 3 680 1 author rukshan manor athna univ ersity of colombo 14 publica tions 7 citations see profile all c ontent f ollo wing this p age was uplo aded b y rukshan manor athna on 04 f ebruar y 2020 the user has r equest\n",
      "\n",
      "Keywords for ML.pdf: algorithm, algorithms, data, learning, machine, ml, program, rules, supervised, unsupervised\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Summary for NLP.pdf:\n",
      "see discussions st ats and author pr ofiles f or this public ation at https www researchgate ne t public ation 328268746 natu ral language processing in artiﬁcial intelligence nlp ai and natu ral language processing algorithms relating to grammar as a foreign language article oct ober 2018 citation 1reads 11 513 1 author javier julian enrique z univ ersitat p olitècnic a de v alència 84 publica tions 29 citations see profile all c ontent f ollo wing this p age was uplo aded b y javier julian enrique z on 13 oct ober 2018 the user has r\n",
      "\n",
      "Keywords for NLP.pdf: al, based, et, framework, language, learning, model, research, sequence, translation\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Summary for NLP_removed.pdf:\n",
      "see discussions st ats and author pr ofiles f or this public ation at https www researchgate ne t public ation 328268746 natu ral language processing in artiﬁcial intelligence nlp ai and natu ral language processing algorithms relating to grammar as a foreign language article oct ober 2018 citation 1reads\n",
      "\n",
      "Keywords for NLP_removed.pdf: al, artificial, framework, intelligence, language, learning, model, research, theory, uai\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Summary for attention.pdf:\n",
      "provided proper attribution is provided google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works attention is all you need ashish vaswani google brain avaswani google comnoam shazeer google brain noam google comniki parmar google research nikip google comjakob uszkoreit google research usz google com llion jones google research llion google comaidan n gomez university of toronto aidan cs toronto edułukasz kaiser google brain lukaszkaiser google com illia polosukhin illia polosukhin gmail com abstract the dominant sequence transduction models are based on complex recurrent or convolutional neural networks that\n",
      "\n",
      "Keywords for attention.pdf: arxiv, attention, input, layer, layers, model, models, self, sequence, transformer\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from PyPDF2 import PdfReader\n",
    "from collections import Counter\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import time\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean the text by removing non-alphabetic characters and converting to lowercase.\"\"\"\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    return text.lower()\n",
    "\n",
    "def sentence_tokenize(text):\n",
    "    \"\"\"Tokenize text into sentences using basic punctuation rules.\"\"\"\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    return [sentence.strip() for sentence in sentences if len(sentence) > 10]  # Ignore very short sentences\n",
    "\n",
    "def word_tokenize(text):\n",
    "    \"\"\"Tokenize text into words.\"\"\"\n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "    return words\n",
    "\n",
    "def score_sentences(text):\n",
    "    \"\"\"Score sentences based on word frequency and other features.\"\"\"\n",
    "    sentences = sentence_tokenize(text)\n",
    "    words = word_tokenize(text) \n",
    "    word_freq = Counter(words)\n",
    "\n",
    "    sentence_scores = {}\n",
    "    for sentence in sentences:\n",
    "        sentence_word_count = word_tokenize(sentence)\n",
    "        score = sum(word_freq[word] for word in sentence_word_count)\n",
    "        sentence_scores[sentence] = score\n",
    "\n",
    "    return sentence_scores\n",
    "\n",
    "\n",
    "\n",
    "def summarize_text(text, num_sentences=5):\n",
    "    \"\"\"Summarize text by extracting the highest-scoring sentences, with a word limit.\"\"\"\n",
    "    # Tokenize the input text to determine its word count\n",
    "    total_word_count = len(word_tokenize(text))\n",
    "    \n",
    "    # Set word limit based on the length of the input text\n",
    "    if total_word_count > 4000:\n",
    "        max_words = 100\n",
    "    elif 2000 <= total_word_count <= 4000:\n",
    "        max_words = 80\n",
    "    else:\n",
    "        max_words = 50\n",
    "\n",
    "    # Score sentences based on their importance\n",
    "    sentence_scores = score_sentences(text)\n",
    "    sorted_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)\n",
    "\n",
    "    \n",
    "    # Add sentences while keeping total word count under max_words\n",
    "    summary_sentences = []\n",
    "    word_count = 0\n",
    "    for sentence in sorted_sentences:\n",
    "        sentence_word_count = len(word_tokenize(sentence))\n",
    "\n",
    "        # If a sentence is longer than the remaining word count, truncate it\n",
    "        if sentence_word_count > max_words - word_count:\n",
    "            remaining_words = max_words - word_count\n",
    "            truncated_sentence = ' '.join(word_tokenize(sentence)[:remaining_words])\n",
    "            summary_sentences.append(truncated_sentence)\n",
    "            break\n",
    "        else:\n",
    "            summary_sentences.append(sentence)\n",
    "            word_count += sentence_word_count\n",
    "            \n",
    "\n",
    "        # Stop adding sentences if the word limit is reached\n",
    "        if word_count >= max_words:\n",
    "            break\n",
    "    \n",
    "    return ' '.join(summary_sentences)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def extract_keywords(text, num_keywords=5):\n",
    "    \"\"\"Extracts top keywords using TF-IDF\"\"\"\n",
    "    try:\n",
    "        # Initialize the TF-IDF vectorizer\n",
    "        tfidf = TfidfVectorizer(max_features=num_keywords, stop_words='english')\n",
    "        tfidf_matrix = tfidf.fit_transform([text])\n",
    "        \n",
    "        # Get feature names (the keywords) and their TF-IDF scores\n",
    "        feature_names = tfidf.get_feature_names_out()\n",
    "        \n",
    "        return list(feature_names)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting keywords: {str(e)}\")\n",
    "        return [\"No Keywords Found\"]\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file using PyPDF2.\"\"\"\n",
    "    try:\n",
    "        text=\"\"\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            reader = PdfReader(file)\n",
    "            # text = \"\"\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:  # Only add non-empty text\n",
    "                    text += page_text\n",
    "                    \n",
    "            if not text.strip():  # Check if no text was extracted\n",
    "                logging.warning(f\"No text extracted from {pdf_path}\")\n",
    "        # print(\"The text is: \",text)\n",
    "        return text  # Return stripped text to avoid empty results\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading {pdf_path}: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "def process_pdf_for_summary_and_keywords(pdf_path, num_sentences=5, top_n=10):\n",
    "    \"\"\"Process a single PDF for summarization and keyword extraction.\"\"\"\n",
    "    pdf_text = extract_text_from_pdf(pdf_path)\n",
    "    # print(\"after the text is extracted::   \",pdf_text)\n",
    "\n",
    "     # Check if the extracted text is valid before proceeding\n",
    "    if not pdf_text:  # If no text was extracted, return an error message\n",
    "        logging.error(f\"Failed to extract text from {pdf_path}\")\n",
    "        return None, None\n",
    "    \n",
    "    cleaned_text = clean_text(pdf_text)\n",
    "    \n",
    "\n",
    "    if not cleaned_text:\n",
    "        logging.error(f\"Cleand text is empty for {pdf_path}\")\n",
    "        return None, None\n",
    "    summary = summarize_text(cleaned_text, num_sentences)\n",
    " \n",
    "    \n",
    "    keywords = extract_keywords(cleaned_text, top_n)\n",
    "    \n",
    "    return summary, keywords\n",
    "\n",
    "def process_multiple_pdfs_for_summary_and_keywords(folder_path, max_workers=4):\n",
    "    \"\"\"Process multiple PDFs in parallel for summarization and keyword extraction.\"\"\"\n",
    "    pdf_summaries_keywords = {}\n",
    "\n",
    "    pdf_files = [f for f in os.listdir(folder_path) if f.endswith('.pdf')]\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_pdf = {}\n",
    "        for pdf_file in pdf_files:\n",
    "            pdf_path = os.path.join(folder_path, pdf_file)\n",
    "            future = executor.submit(process_pdf_for_summary_and_keywords, pdf_path)\n",
    "            future_to_pdf[future] = pdf_file\n",
    "\n",
    "        for future in as_completed(future_to_pdf):\n",
    "            pdf_name = future_to_pdf[future]\n",
    "            try:\n",
    "                summary, keywords = future.result()\n",
    "                if summary is None and keywords is None:\n",
    "                    pdf_summaries_keywords[pdf_name] = {\"error\": \"Failed to extract text\"}\n",
    "                else:\n",
    "                    pdf_summaries_keywords[pdf_name] = {\n",
    "                        \"summary\": summary,\n",
    "                        \"keywords\": keywords\n",
    "                    }\n",
    "            except Exception as e:\n",
    "                pdf_summaries_keywords[pdf_name] = {\n",
    "                    \"error\": str(e)\n",
    "                }\n",
    "\n",
    "    return pdf_summaries_keywords\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def process_pdf(pdf_name, folder_path):\n",
    "    \"\"\"Simulates processing a PDF by extracting metadata, summarizing, and updating MongoDB.\"\"\"\n",
    "    start_time = time.time()  # Start the timer for this document\n",
    "\n",
    "    # Store initial metadata\n",
    "    pdf_id = store_metadata_in_mongodb(pdf_name, folder_path)\n",
    "    \n",
    "    if pdf_id is None:\n",
    "        logging.error(f\"Failed to store metadata for {pdf_name}\")\n",
    "        return\n",
    "    \n",
    "       # Extract text, summarize and get keywords for the current PDF\n",
    "    pdf_path = os.path.join(folder_path, pdf_name)\n",
    "    summary, keywords = process_pdf_for_summary_and_keywords(pdf_path)\n",
    "    \n",
    "    if summary is None and keywords is None:\n",
    "        logging.error(f\"Failed to extract summary and keywords for {pdf_name}\")\n",
    "        return\n",
    "\n",
    "    # Simulate document processing (e.g., text extraction, summarization, keyword extraction)\n",
    "    # In real code, replace this with actual logic\n",
    "    # summary = f\"Summary of {pdf_name}\"  # Placeholder for actual summary\n",
    "    # summary = result['summary']\n",
    "    # keywords= result['keywords']\n",
    "    # keywords = [\"keyword1\", \"keyword2\"]  # Placeholder for actual keywords\n",
    "\n",
    "    # Update MongoDB with summary and keywords\n",
    "    update_mongodb_with_summary_and_keywords(pdf_id, summary, keywords)\n",
    "\n",
    "    # Measure time taken for this document\n",
    "    time_taken = time.time() - start_time\n",
    "    logging.info(f\"Processed {pdf_name} in {time_taken:.2f} seconds\")\n",
    "\n",
    "    return time_taken\n",
    "\n",
    "\n",
    "\n",
    "def process_pdfs_concurrently(folder_path, max_workers=5):\n",
    "    \"\"\"Process multiple PDFs concurrently.\"\"\"\n",
    "    pdf_files = [f for f in os.listdir(folder_path) if f.endswith('.pdf')]\n",
    "    \n",
    "    total_start_time = time.time()  # Start time for all PDFs\n",
    "    total_time_taken = 0\n",
    "    futures = []\n",
    "\n",
    "    # Use ThreadPoolExecutor for concurrency\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        for pdf_file in pdf_files:\n",
    "            # Submit each PDF to the executor\n",
    "            future = executor.submit(process_pdf, pdf_file, folder_path)\n",
    "            futures.append(future)\n",
    "\n",
    "        # Wait for all futures to complete\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                time_taken = future.result()\n",
    "                total_time_taken += time_taken\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing PDF: {str(e)}\")\n",
    "\n",
    "    # Calculate total time for all PDFs\n",
    "    total_time = time.time() - total_start_time\n",
    "    avg_time_per_doc = total_time_taken / len(pdf_files) if pdf_files else 0\n",
    "\n",
    "    logging.info(f\"Total processing time for all PDFs: {total_time:.2f} seconds\")\n",
    "    logging.info(f\"Average time per document: {avg_time_per_doc:.2f} seconds\")\n",
    "    logging.info(f\"Processed {len(pdf_files)} documents in parallel\")\n",
    "    \n",
    "    # Return performance metrics\n",
    "    return {\n",
    "        \"total_time\": total_time,\n",
    "        \"average_time_per_doc\": avg_time_per_doc,\n",
    "        \"total_documents\": len(pdf_files)\n",
    "    }\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Folder path where the PDF files are stored\n",
    "folder_path = 'D:\\AI_ML\\RAG\\data'\n",
    "\n",
    "# Process all PDFs in the folder for summarization and keyword extraction\n",
    "all_pdf_results = process_multiple_pdfs_for_summary_and_keywords(folder_path)\n",
    "                            \n",
    "# Print results for each PDF\n",
    "for pdf_name, result in all_pdf_results.items():\n",
    "    if \"error\" in result:\n",
    "        print(f\"Error processing {pdf_name}: {result['error']}\")\n",
    "    else:\n",
    "        print(f\"Summary for {pdf_name}:\\n{result['summary']}\\n\")\n",
    "        print(f\"Keywords for {pdf_name}: {', '.join(result['keywords'])}\\n\")\n",
    "        print(\"=\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from pymongo import MongoClient\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# MongoDB connection string with credentials\n",
    "connection_string = os.getenv(\"MONGO_URL\")\n",
    "\n",
    "# Connect to the MongoDB client\n",
    "client = MongoClient(connection_string)\n",
    "\n",
    "# Select the database and collection\n",
    "db = client[\"pdf_database\"]\n",
    "collection = db[\"pdf_documents\"]\n",
    "\n",
    "# Setting up logging for error tracking\n",
    "logging.basicConfig(filename='pdf_processing_errors.log', level=logging.INFO)\n",
    "\n",
    "# Check MongoDB connection\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    \n",
    "    logging.info(\"MongoDB connection successful!\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error connecting to MongoDB: {str(e)}\")\n",
    "    exit(1)\n",
    "\n",
    "def store_metadata_in_mongodb(pdf_name, folder_path):\n",
    "    \"\"\"Store initial metadata about the PDF file in MongoDB.\"\"\"\n",
    "    pdf_path = os.path.join(folder_path, pdf_name)\n",
    "    try:\n",
    "        file_stats = os.stat(pdf_path)\n",
    "        pdf_metadata = {\n",
    "            \"pdf_name\": pdf_name,\n",
    "            \"file_size\": file_stats.st_size,\n",
    "            \"upload_date\": datetime.now(),\n",
    "            \"status\": \"pending\",\n",
    "            \"summary\": None,\n",
    "            \"keywords\": None\n",
    "        }\n",
    "        result = collection.insert_one(pdf_metadata)\n",
    "        logging.info(f\"Successfully stored metadata for {pdf_name} in MongoDB with ID: {result.inserted_id}\")\n",
    "        return result.inserted_id\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error storing metadata for {pdf_name}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def update_mongodb_with_summary_and_keywords(pdf_id, summary, keywords):\n",
    "    \"\"\"Update MongoDB document with the summary and keywords.\"\"\"\n",
    "    try:\n",
    "        collection.update_one(\n",
    "            {\"_id\": pdf_id},\n",
    "            {\"$set\": {\n",
    "                \"summary\": summary,\n",
    "                \"keywords\": keywords,\n",
    "                \"status\": \"processed\"\n",
    "            }}\n",
    "        )\n",
    "        logging.info(f\"Successfully updated MongoDB document with ID: {pdf_id}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error updating MongoDB for document ID {pdf_id}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total_time': 2.601963758468628, 'average_time_per_doc': 1.6288970947265624, 'total_documents': 5}\n"
     ]
    }
   ],
   "source": [
    "data =process_pdfs_concurrently(folder_path, max_workers=5)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
